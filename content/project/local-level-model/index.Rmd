---
title: Local Level Model
author: admin
date: '2021-07-01'
summary: 'In this short project, we present some exercices regarding the Local Level Model. We apply the Kalman filter to simulated data and illustarte the pile-up problem that may occur for maximum likelihood estimation and small sample size.'
slug: []
categories:
#  - Macroeconometrics
#  - Local Level Model
#  - Kalman Filter
tags:
  - Macroeconometrics
  - Local Level Model
  - Kalman Filter
links:
- icon: twitter
  icon_pack: fab
  name: Follow
  url: https://twitter.com/georgecushen
slides: ""
math: true
description: Description for the page
bibliography: [bibliography.bib]
editor_options: 
  chunk_output_type: console
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = FALSE, fig.height = 4, warning = FALSE, out.width = "100%"
)

if (!require("ggplot2")) {install.packages("ggplot2"); library('ggplot2')}
if (!require("tidyverse")) {install.packages("tidyverse"); library('tidyverse')}
if (!require("matrixStats")) {install.packages("matrixStats"); library('matrixStats')}
if (!require("tidymodels")) {install.packages("tidymodels"); library('tidymodels')}
if (!require("lme4")) {install.packages("lme4"); library('lme4')}
if (!require("haven")) {install.packages("haven"); library('haven')}
if (!require("miceadds")) {install.packages("miceadds"); library('miceadds')}
if (!require("rmarkdown")) {install.packages("rmarkdown"); library('rmarkdown')}
if (!require("reshape2")) {install.packages("reshape2"); library('reshape2')}
if (!require("plotly")) {install.packages("plotly"); library('plotly')}
if (!require("knitr")) {install.packages("knitr"); library('knitr')}
if (!require("kableExtra")) {install.packages("kableExtra"); library('kableExtra')}
if (!require("htmltools")) {install.packages("htmltools"); library('htmltools')}
```


```{r}
# Move this css tag outside the chunk to control the width of text
# on the page.
# <style type="text/css">
# .main-container {
#   max-width: 1000px;
#   margin-left: auto;
#   margin-right: auto;
# }
# </style>
```

# Preliminaries

In this short project, we will apply and discuss some properties of the local level model. First, we use simulated time series to apply the Kalman Filter. Second, we discuss the properties of the Maximum Likelihood Estimator (MLE) in a small Monte Carlo simulation exercise. Within this framework, we will also analyze the pile-up problem of the MLE estimator.


# Local Level Model

The Local Level Model is based on two equations, namely the measurement equation and the transition (or state) equation. The measurement equation represents a noisy signal that is observed by the agent. The true underlying data generation process in the form of the transition equation is not directly observed by the agent. An example for this framework could be as follows. When professional analysts have to forecast GDP, they might not observe a perfect signal of GDP - presumably, the data they observe contains some measurement inaccuracies. Often, the GDP data is subject to continuous revisions in the first few quarters after the first publication of the data. Using this noisy signal of GDP, the forecasters then try to "filter out" the noise in an optimal way to forecast GDP according their assumption of the underlying data generating process. 


The framework of an exemplary Local Level Model is presented below.
\begin{align*}
  &  \text{Measurement Equation: }   y_t = \mu_t + \varepsilon_t,  \varepsilon_t \sim\left(0, \sigma_\varepsilon^2 \right)\\
  &  \text{Transition Equation: }   \mu_{t+1} = \mu_t + \eta_t,  \eta_t \sim\left(0, \sigma_\eta^2 \right)\\
\end{align*}
Note that this framework could be easily extended to a case where the agents receive multiple signals, observe different exogenous variables, allow for covariances among the noises in the measurement and transition equation, etc. Also, we consider here the special case where the transition equation follows a random walk process. Coming back to the example from above, the agent is interested in guessing the distribution of the underlying data generating process, given his observations of the time series $Y_t = y_1, y_2, \dots, y_T$, and using this signal to predict the future outcome. Therefore, we are interested in the distribution of the predicted signal density $f(\mu_{t+1}|Y_t) \equiv N \left( a_{t+1} , p_{t+1}\right)$ (as all the noise is normal, the predicted signal will also be normally distributed). $a_{t+1}$ is simple the conditional expectation of the future outcome of $\mu_t$ (GDP forecast in the next quarter):
$$
a_{t+1} = \mathbb{E}(\mu_{t+1} | Y_t) = \mathbb{E}( \mu_t + \eta_t| Y_t) = a_{t|t}
$$
where $a_{t|t}$ is simply the result of the Kalman filter. Note that for a transition equation with  $\mu_{t+1} = \rho \mu_t + \eta_t, \rho < 1$, $a_{t+1}= \rho a_{t|t}$. Further, we have that 
$$
p_{t+1} = \mathbb{V}(\mu_t + \eta_t| Y_t) =  p_{t|t} + \sigma_\eta^2
$$
Now, one can show that the result of the signal extraction (Kalman Filter) can be obtained by recursively solving the following system of equations (given a starting value for $a_1$ and $p_1$).
\begin{align*}
  & v_t         = y_t - a_t \\
  & k_t         = \frac{p_t}{p_t + \sigma_{\varepsilon}^2} \\
  & a_{t|t}     = a_t + k_t v_t \\
  & p_{t|t}     = k_t \sigma_{\varepsilon}^2  \\
  & a_{t+1}     = a_{t|t} \\
  & p_{t+1}     = p_{t|t} + \sigma_{\eta}^2
\end{align*}
Given some starting values, we are now ready to calculate the Kalman Fitler and the predicted signal.


# Kalman Filter

In this section, we simply simulate several time series with different parameter values. In detail, we  use the model set-up described above together with initial conditions of $\mu_1 \sim N (a_1 = 0, p_1 = 10^7)$ and variances $\sigma_{\varepsilon}^2 = 1$ and $\sigma_{\eta}^2 = q$ with $q\in \{10,1,0.1,0.001\}$.


```{r, include = FALSE}
# clear variables:
rm(list=ls(all=TRUE))

# Simulate all time series
# ------------------------

# Initial conditions
a1 <- 0
p1 <- 10^7

# Starting value for mu
mu.t <- c()
mu.t[1] <- rnorm(1,a1,sqrt(p1))

# Different variances for eta
q <- c(10,1,0.1,0.001)

# Other parameters
var.vareps <- 1
rho <- 1
t <- 100


# State/Transition Equation: Data Generating Process "x": AR(1) process 
# ______________________________________________________________________

# Pre-allocate dataframe
df.ts <- data.frame(matrix(NA,nrow = t*(NROW(q)), ncol = 4))
colnames(df.ts) <- c("t","q","y.t","mu.t")

x <- 1
for(var in q){
  
  
  # DATA GENERATING PROCESS (TRANSITION EQUATION)
  for(i in 2:t){
    mu.t[i] = mu.t[i-1] + rnorm(mean = 0, sd = sqrt(var), n = 1)
  }

  # NOISY SIGNAL OBSERVED (MEASUREMENT EQUATION)
  y.t <- c()
  for(i in 1:t){
    y.t[i] = mu.t[i] + rnorm(mean = 0, sd = 1, n = 1)
  }
 
  # indexes for dataframe in long format
  i1 <- 1 + (t*(x-1))
  i2 <- t*x
  
  df.ts[i1:i2,] <- cbind(c(1:t),rep(var,t),y.t,mu.t)
  
  x <- x + 1
}

# put df in long format
df.ts <- df.ts %>%
  gather(key = key, value = value, y.t:mu.t,-t)

# Plot the different time series
figures <- c()

x <- 1
for(var in q){
  
  if(x < NROW(q)){
    bin <- F
  } else {
    bin <- T
  }
  
  fig <- df.ts %>%
      dplyr::filter(q == var) %>%
      print(var) %>%
      plot_ly(.,.x = ~t,y = ~value,color = ~key, type = "scatter",mode = "lines",legendgroup = ~key, showlegend = bin, colors = c("#003f5c","#ff6361")) 
  
  xtitle <- paste('t <br> q =',var)
  fig <- fig %>% layout(title=TeX(" \\text{Actual Data } \\mu_t \\text{ and Observed Noisy Signal }  y_t "), xaxis = list(title = xtitle)) 
  
  figures[x] <- fig   
  x <- x + 1
}


plot <- subplot(figures, nrows = 1, shareX = T, shareY = T)  %>% 
      config(mathjax = 'cdn')
```

Note that we simulated ``r t`` time periods.
```{r, echo = FALSE}
plot
```

For the Kalmna Filter that we will calculate below, a parameter of interest is the signal-to-noise ratio. This ratio is defined given by $\frac{\sigma_\eta^2}{\sigma_\varepsilon^2}$. In our example above, the variance of the data the agent observes, $\sigma_\varepsilon^2$, is fixed to 1. However, the variance of the data generating process, $\sigma_\eta^2$, varies. As $\sigma_\varepsilon^2$ is fixed to 1, $q$ directly indicates the signal-to-noise ratio. The higher this ratio is, the more of the variation the agent observes in the time series $y_t$ actually stemms from variations in the true data generating process. In that sense, the signal observed is very informative about the true data generating process. In other words, the signal $y_t$ observed is a very good indicator of the true $mu_t$. This is represented in the graph - The higher the signal-to-noise ratio, the closer the two time series are. Consequently, the more informative the signal observed by the agent is about the $mu_t$.


Using the generated time series of the signal, we can now calcualte the Kalman filter as described below. Some code snippets are attached as well.